{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8615111,"sourceType":"datasetVersion","datasetId":5156236},{"sourceId":8631273,"sourceType":"datasetVersion","datasetId":5168131},{"sourceId":10558461,"sourceType":"datasetVersion","datasetId":6532163},{"sourceId":244149,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":208570,"modelId":230259}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"dataset_train = torch.load('/kaggle/input/orientation-dataset/train_dataset_all.pt')\ndataset_valid = torch.load('/kaggle/input/orientation-dataset/train_dataset_all.pt')","metadata":{"execution":{"iopub.status.busy":"2025-01-28T13:19:49.003431Z","iopub.execute_input":"2025-01-28T13:19:49.003766Z","iopub.status.idle":"2025-01-28T13:20:03.125159Z","shell.execute_reply.started":"2025-01-28T13:19:49.003743Z","shell.execute_reply":"2025-01-28T13:20:03.124153Z"},"trusted":true},"outputs":[{"name":"stderr","text":"<ipython-input-30-5714f3b20d5f>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  dataset_train = torch.load('/kaggle/input/orientation-dataset/train_dataset_all.pt')\n<ipython-input-30-5714f3b20d5f>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  dataset_valid = torch.load('/kaggle/input/orientation-dataset/train_dataset_all.pt')\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom typing import List, Union\nfrom transformers import AutoTokenizer, AutoModel, PreTrainedModel\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, BertForSequenceClassification\nimport pandas as pd\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\ndef evaluate(dataset: Dataset, model: PreTrainedModel, device: torch.device = torch.device('cpu'), plot: bool = False):\n    '''\n    Evaluates the model on the given dataset.\n    \n    Parameters:\n        dataset: Dataset\n            The dataset to evaluate on.\n        model: PreTrainedModel\n            The model to evaluate.\n        device: torch.device\n            The device to use.\n        plot: bool\n    '''\n    \n    model.eval()\n    loader = DataLoader(dataset, batch_size=16, shuffle=False)\n    correct_labels = []\n    model_predictions = []\n    probs = []\n    attentions = []\n    embeddings = []\n    texts = []\n    with torch.no_grad():\n        for batch in loader:\n            id_, speaker, sex, text, text_en, embedding, attention_mask, label = batch\n            texts.extend(text_en)\n            embedding = embedding.to(device)\n            attention_mask = attention_mask.to(device).squeeze(1)\n            assert(attention_mask.nonzero().size() == embedding.nonzero().size())\n            label = label.to(device)\n            model_output = model(input_ids=embedding, labels=label, attention_mask=attention_mask, output_attentions=True)\n            embeddings.extend(embedding.cpu())\n            \n            \n            attention = torch.mean(model_output.attentions[-1], dim=1).squeeze()[:,0]\n           \n           \n            attentions.extend(attention.cpu().numpy())\n            logits = model_output.logits\n            \n            prob = torch.max(torch.softmax(logits, dim=1), dim=1)\n            \n            probs.extend(prob.values.cpu())\n            predictions = torch.argmax(logits, dim=1)\n            correct_labels.extend(label.cpu().numpy())\n            model_predictions.extend(predictions.cpu().numpy())\n\n    accuracy = accuracy_score(correct_labels, model_predictions)\n    print(f'Accuracy: {accuracy}')\n    print(f'Confusion matrix:\\n{confusion_matrix(correct_labels, model_predictions)}')\n    \n    return correct_labels, model_predictions, probs, attentions, embeddings, texts\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-01-28T13:20:07.542007Z","iopub.execute_input":"2025-01-28T13:20:07.542344Z","iopub.status.idle":"2025-01-28T13:20:07.554240Z","shell.execute_reply.started":"2025-01-28T13:20:07.542316Z","shell.execute_reply":"2025-01-28T13:20:07.553151Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\nmodel = torch.load('/kaggle/input/distilbertforattention/pytorch/default/1/distilbert_cased_en_novi.pt')","metadata":{"execution":{"iopub.status.busy":"2025-01-28T13:20:14.117473Z","iopub.execute_input":"2025-01-28T13:20:14.117799Z","iopub.status.idle":"2025-01-28T13:20:14.454876Z","shell.execute_reply.started":"2025-01-28T13:20:14.117773Z","shell.execute_reply":"2025-01-28T13:20:14.453962Z"},"trusted":true},"outputs":[{"name":"stderr","text":"<ipython-input-32-5f85fd89954e>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load('/kaggle/input/distilbertforattention/pytorch/default/1/distilbert_cased_en_novi.pt')\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"output = evaluate(dataset_valid,model, 'cuda:0')","metadata":{"execution":{"iopub.status.busy":"2025-01-28T13:20:17.909367Z","iopub.execute_input":"2025-01-28T13:20:17.909669Z","iopub.status.idle":"2025-01-28T13:24:59.010233Z","shell.execute_reply.started":"2025-01-28T13:20:17.909635Z","shell.execute_reply":"2025-01-28T13:24:59.009196Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Accuracy: 0.8838941771990704\nConfusion matrix:\n[[10812  1945]\n [ 1702 16952]]\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"labels, predictions, probs, attentions, embeddings, old_texts = output","metadata":{"execution":{"iopub.status.busy":"2025-01-28T13:25:09.608375Z","iopub.execute_input":"2025-01-28T13:25:09.608688Z","iopub.status.idle":"2025-01-28T13:25:09.613445Z","shell.execute_reply.started":"2025-01-28T13:25:09.608666Z","shell.execute_reply":"2025-01-28T13:25:09.612466Z"},"trusted":true},"outputs":[],"execution_count":35},{"cell_type":"code","source":"labels = np.array(labels)\npredictions = np.array(predictions)\nprobs = np.array(probs)\nattentions = np.array(attentions)\nembeddings = np.array(embeddings)\nprint(attentions.shape)\nprint(attentions.reshape(len(labels),-1).shape)","metadata":{"execution":{"iopub.status.busy":"2025-01-28T13:25:13.418464Z","iopub.execute_input":"2025-01-28T13:25:13.418780Z","iopub.status.idle":"2025-01-28T13:25:13.784723Z","shell.execute_reply.started":"2025-01-28T13:25:13.418758Z","shell.execute_reply":"2025-01-28T13:25:13.783920Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(31411, 512)\n(31411, 512)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"ind = labels == predictions\nprint(len(ind))\nlabels = labels[ind]\npredictions = predictions[ind]\nprobs = probs[ind]\nattentions = attentions[ind]\nembeddings = embeddings[ind]\ntexts = []\nfor i, cond in enumerate(ind):\n    if cond:\n        texts.append(old_texts[i])\n        ","metadata":{"execution":{"iopub.status.busy":"2025-01-28T13:25:19.567909Z","iopub.execute_input":"2025-01-28T13:25:19.568284Z","iopub.status.idle":"2025-01-28T13:25:19.643671Z","shell.execute_reply.started":"2025-01-28T13:25:19.568238Z","shell.execute_reply":"2025-01-28T13:25:19.642847Z"},"trusted":true},"outputs":[{"name":"stdout","text":"31411\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"most_left = torch.topk(torch.tensor(probs), 5000, largest=False).indices.numpy()\ngood = most_left[6]\nmost_important = torch.topk( torch.tensor(attentions[good]), 15).indices.numpy()\n\nprint(tokenizer.decode(embeddings[good][most_important]))\nfor ind in most_left[6:10]:\n    print(texts[ind])\nmost_important = torch.topk( torch.tensor(attentions[most_left]), 5).indices.numpy()\nprint(most_important.shape)\nprint(most_left.shape)\n\ncounter_l = {}\nfor ind, att in zip(most_left, most_important):\n    words = tokenizer.decode(embeddings[ind][att]).lower().split(' ')\n    for word in words:\n        if '[SEP]' in word:\n            word = word.replace('[SEP]', '')\n        if '....' in word:\n            word = word.replace('....', '')\n        if '...' in word:\n            word = word.replace('...', '')\n        if '..' in word:\n            word = word.replace('..', '')\n        if '.' in word:\n            word = word.replace('.', '')\n        if ',' in word:\n            word = word.replace(',', '')\n        if '–' in word:\n            word = word.replace('–', '')\n        if '?' in word:\n            word = word.replace('?', '')\n        if '!' in word:\n            word = word.replace('!', '')\n        if '\"\"' in word:\n            word = word.replace('\"\"', '')\n        if '</s>' in word:\n            word = word.replace('</s>', '')\n        if '<s>' in word:\n            word = word.replace('<s>', '')\n        if '[sep]' in word:\n            word = word.replace('[sep]', '')\n        if '[cls]' in word:\n            word = word.replace('[cls]', '')\n        if '>' in word:\n            word = word.replace('>', '')\n        if word not in counter_l:\n            counter_l[word] = 1\n        else:\n            counter_l[word] += 1\n            \nprint()\nmost_popular_left = list(map(lambda x: x,sorted(counter_l.items(), key=lambda x: -x[1])))\n\nprint(most_popular_left[:100])","metadata":{"execution":{"iopub.status.busy":"2025-01-28T13:25:25.908189Z","iopub.execute_input":"2025-01-28T13:25:25.908521Z","iopub.status.idle":"2025-01-28T13:25:26.038459Z","shell.execute_reply.started":"2025-01-28T13:25:25.908498Z","shell.execute_reply":"2025-01-28T13:25:26.037514Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[SEP] President.lsson. borders borders Sweden infection [CLS].am Swedish border.\nMadam President, I would like Thank you, Annika Qarlsson, for a very urgent question! <p> We should not repeat the mistake that took place last spring. We in Sweden, as I mentioned earlier, have consistently been in favour of openness in relation to borders. We are also leaning on the evidence-based evidence available from the European Disease Control Agency and from our own public health authority, which says that border closures are not an effective means of combating the spread of infection. <p> The Commission has also said this. The Commission says that if borders are to be closed now, a common approach must be found to restrictions and to the level to be taken. <p> On the Swedish side, we are pushing for there to be no border closures at all. We don't have a lockdown yet. However, we have new shutdowns which are worrying, although we do not have production units that are closing down. What we are doing now is trying to get the industry and production facilities to be open and to find solutions for this.\nThank you, Mr President. Minister, thank you very much. I am extremely pleased that the various laws and regulations have come together as a specific law that deals specifically with the issue of cyber security. I would also comfort my colleague, Mr Sults, that the importance of electromagnetic waves will be reduced if we cover Estonia by means of an optical network over the next five years, because the need for all these Wi-Fis and copper cables is smaller. However, I ask here whether, apart from the general guidelines that are in the law, there will be some regulations that specifically address the technical rules on how one or the other company should ensure cybersecurity. Or are we still left with the general guidelines that exist today?\nMadam President! It was a good thing that this matter of fisheries management fees came up. In the role of Vice-President of the Committee on Agriculture and Forestry myself, I have held three negotiations with the government partners on this issue, and the position of us blues is quite clear and clear: we will not accept the extension of the fisheries management fee to 65-year-olds, because we all know that grandmothers and grandfathers and grandchildren are enthusiastically fishing. Of course, they themselves do fishing, but there is a silent transfer of data then for those posterity. It is an extremely important issue for many others. And if you think of the fish management fee as just money, for us it is more a matter of principle than a question of money. If you think that these money from fisheries management fees is used to plant fish and manage fish stocks, then I would urge that controls be directed at those fishermen who need it to be paid, and then get this money for the management of fish stocks. But we don't want to open a fish-management fee.\nMr President, the bill tabled by Mrs Dierick and a number of colleagues on the Business Committee proposes that the Coordinated Universal Time (UTC) be taken as the basis for the legal time in Belgium and not based on the reference time determined by the rotation of the earth and this to remove legal uncertainties. <p> The legislative proposal was adopted unanimously after amendment of Articles 2, 4 and 5. <p> Mr President, I will take the floor again very briefly. <p> This is a proposal for a resolution containing recommendations on the payment behaviour of the federal government. Apparently, the deadlines are not really respected by, on the one hand, the loopholes in the legislation and, on the other hand, the lack of information. That is why a number of colleagues have tabled a motion for a resolution to speed up payments and to invite officials to the committee if necessary to answer for long delays in payments. <p> This motion for a resolution was also adopted unanimously in committee.\n(5000, 5)\n(5000,)\n\n[('', 7580), ('president', 819), ('minister', 795), (\"'\", 453), ('government', 268), ('mr', 149), ('is', 148), ('state', 131), ('secretary', 115), ('municipalities', 109), ('norway', 107), ('are', 107), ('am', 105), ('will', 105), ('the', 98), ('chamber', 90), ('estonia', 86), ('house', 86), ('norwegian', 77), ('members', 74), ('estonian', 74), ('right', 72), ('prime', 68), ('chairman', 62), ('sweden', 58), ('portuguese', 54), ('netherlands', 54), ('republic', 52), ('does', 51), ('country', 51), ('representative', 51), ('have', 50), ('czech', 48), ('ministry', 48), ('people', 48), ('<', 43), ('national', 43), ('parliament', 41), ('party', 41), ('swedish', 39), ('socialist', 39), ('must', 37), ('has', 37), ('mrs', 37), ('progress', 33), ('but', 32), ('finland', 30), ('senator', 30), ('cabinet', 27), ('group', 27), ('can', 26), ('denmark', 25), ('municipality', 25), ('deputies', 25), ('eu', 25), ('member', 25), ('left', 25), ('m', 24), ('waitress', 24), ('bill', 23), ('council', 23), ('colleagues', 23), ('my', 23), ('amendment', 22), ('dutch', 22), ('ministers', 22), ('danish', 22), ('leader', 21), ('our', 21), ('now', 21), ('you', 21), ('christian', 20), ('deputy', 19), ('union', 19), ('s', 19), ('portugal', 19), ('that', 19), ('want', 19), ('do', 19), ('department', 19), ('re', 18), ('citizens', 17), ('agree', 17), ('farmers', 17), ('committee', 17), ('presidentam', 17), ('slovenian', 17), ('interior', 17), ('coalition', 16), ('still', 16), ('european', 16), ('debate', 16), ('commission', 16), ('need', 16), ('federal', 15), ('nordic', 15), ('think', 15), ('tallinn', 15), ('speaker', 15), ('question', 15)]\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"most_right = torch.topk(torch.tensor(probs), 5000, largest=True).indices.numpy()\n\nmost_important = torch.topk( torch.tensor(attentions[most_right]), 5).indices.numpy()\n\ncounter_r = {}\nfor ind, att in zip(most_right, most_important):\n    words = tokenizer.decode(embeddings[ind][att]).lower().split(' ')\n    \n    for word in words:\n        if '[SEP]' in word:\n            word = word.replace('[SEP]', '')\n        if '....' in word:\n            word = word.replace('....', '')\n        if '...' in word:\n            word = word.replace('...', '')\n        if '..' in word:\n            word = word.replace('..', '')\n        if '.' in word:\n            word = word.replace('.', '')\n        if ',' in word:\n            word = word.replace(',', '')\n        if '–' in word:\n            word = word.replace('–', '')\n        if '?' in word:\n            word = word.replace('?', '')\n        if '!' in word:\n            word = word.replace('!', '')\n        if '\"\"' in word:\n            word = word.replace('\"\"', '')\n        if '</s>' in word:\n            word = word.replace('</s>', '')\n        if '<s>' in word:\n            word = word.replace('<s>', '')\n        if '[sep]' in word:\n            word = word.replace('[sep]', '')\n        if '[cls]' in word:\n            word = word.replace('[cls]', '')\n        if '>' in word:\n            word = word.replace('>', '')\n        if word not in counter_r:\n            counter_r[word] = 1\n        else:\n            counter_r[word] += 1\n            \n\nmost_popular_right = list(map(lambda x: x,sorted(counter_r.items(), key=lambda x: -x[1])))\nprint(most_popular_right[:100])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T13:25:30.784134Z","iopub.execute_input":"2025-01-28T13:25:30.784418Z","iopub.status.idle":"2025-01-28T13:25:30.907327Z","shell.execute_reply.started":"2025-01-28T13:25:30.784398Z","shell.execute_reply":"2025-01-28T13:25:30.906476Z"}},"outputs":[{"name":"stdout","text":"[('', 7074), ('will', 1443), ('minister', 1119), ('my', 1089), ('<', 922), ('government', 769), ('right', 736), ('are', 686), ('is', 611), ('the', 571), ('friend', 520), ('secretary', 515), ('noble', 316), ('our', 297), ('have', 243), ('does', 213), ('prime', 174), ('has', 148), ('thank', 142), (\"'\", 141), ('can', 134), ('chancellor', 124), ('salute', 113), ('that', 108), ('department', 106), ('now', 99), ('agree', 95), ('we', 92), ('must', 88), ('lords', 85), ('still', 82), ('you', 81), ('but', 66), ('he', 65), ('dear', 58), ('am', 57), ('need', 57), ('p', 55), ('want', 52), ('why', 52), ('president', 50), ('to', 50), ('ho', 49), ('all', 44), ('continue', 44), ('wish', 38), ('ministry', 38), ('home', 38), ('as', 33), ('very', 33), ('i', 32), ('ministers', 32), ('re', 32), ('ask', 31), ('country', 29), ('leader', 29), ('beloved', 28), ('assure', 28), ('this', 27), ('do', 26), ('what', 25), ('going', 24), ('yet', 23), ('much', 22), ('know', 21), ('mr', 21), ('and', 21), ('nation', 21), ('s', 20), ('lady', 20), ('could', 18), ('be', 18), ('already', 18), ('m', 17), ('marshal', 17), ('were', 17), ('she', 16), ('cuts', 16), ('being', 16), ('reassure', 15), ('hope', 15), ('how', 15), ('of', 15), ('should', 14), ('continues', 14), ('was', 14), ('needs', 14), ('welcome', 13), ('think', 13), ('constituency', 13), ('with', 13), ('state', 13), ('friends', 12), ('about', 12), ('believe', 12), ('new', 11), ('urge', 11), ('expect', 11), ('house', 11), ('please', 11)]\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"w = 'treasury'\nprint(counter_l[w])\nprint(counter_r[w])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T13:25:34.633841Z","iopub.execute_input":"2025-01-28T13:25:34.634132Z","iopub.status.idle":"2025-01-28T13:25:34.639219Z","shell.execute_reply.started":"2025-01-28T13:25:34.634111Z","shell.execute_reply":"2025-01-28T13:25:34.638326Z"}},"outputs":[{"name":"stdout","text":"1\n7\n","output_type":"stream"}],"execution_count":40}]}