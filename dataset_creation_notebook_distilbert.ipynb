{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from typing import List, Union\n",
    "from transformers import AutoTokenizer, AutoModel, DistilBertForSequenceClassification, DistilBertTokenizer, BertForSequenceClassification\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                ids: List[str], \n",
    "                speakers: List[str], \n",
    "                sexes: List[str], \n",
    "                texts: List[str], \n",
    "                texts_en: List[str], \n",
    "                labels: List[bool],\n",
    "                device: torch.device = torch.device('cpu'),\n",
    "                model_name: str = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
    "                max_length: int = 512\n",
    "        ):\n",
    "        assert len(ids) == len(speakers) == len(sexes) == len(texts) == len(texts_en) == len(labels)\n",
    "        self.ids = []\n",
    "        self.speakers = []\n",
    "        self.sexes = []\n",
    "        self.texts = []\n",
    "        self.texts_en = []\n",
    "        self.embeddings = []\n",
    "        self.attention_masks = []\n",
    "        self.labels = []\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        for i in range(len(ids)):\n",
    "            inputs = self.tokenizer(texts[i], add_special_tokens=True, return_tensors='pt', padding='max_length',max_length=max_length)\n",
    "            if inputs['input_ids'].shape[1] <= max_length:\n",
    "                inputs = self.tokenizer(texts_en[i], add_special_tokens=True, return_tensors='pt', padding='max_length',max_length=max_length)\n",
    "                self.ids.append(ids[i])\n",
    "                self.speakers.append(speakers[i])\n",
    "                self.sexes.append(sexes[i])\n",
    "                self.texts.append(texts[i])\n",
    "                self.texts_en.append(texts_en[i])\n",
    "                self.embeddings.append(inputs['input_ids'][0])\n",
    "                self.attention_masks.append(inputs['attention_mask'])\n",
    "                self.labels.append(torch.tensor((labels[i]), dtype=torch.long))\n",
    "                \n",
    "        print(f'Loaded {len(self.ids)}/{len(ids)} samples.')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.ids[index], self.speakers[index], self.sexes[index], self.texts[index], \\\n",
    "                self.texts_en[index], self.embeddings[index][:512].to(self.device), self.attention_masks[index][0][:512].to(self.device), self.labels[index]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def set_device(self, device: torch.device):\n",
    "        '''\n",
    "        Sets the device to the given device.\n",
    "        '''\n",
    "        self.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 95/2935 samples.\n",
      "Processed orientation-hu-train.tsv, created train, val, and test datasets of size 77, 9, and 9 respectively.\n",
      "Loaded 140/3907 samples.\n",
      "Processed orientation-bg-train.tsv, created train, val, and test datasets of size 112, 14, and 14 respectively.\n",
      "Loaded 357/1249 samples.\n",
      "Processed orientation-ba-train.tsv, created train, val, and test datasets of size 286, 36, and 35 respectively.\n",
      "Loaded 16/850 samples.\n",
      "Processed orientation-es-ga-train.tsv, created train, val, and test datasets of size 13, 2, and 1 respectively.\n",
      "Loaded 5041/16138 samples.\n",
      "Processed orientation-tr-train.tsv, created train, val, and test datasets of size 4033, 504, and 504 respectively.\n",
      "Loaded 59/5639 samples.\n",
      "Processed orientation-gr-train.tsv, created train, val, and test datasets of size 48, 6, and 5 respectively.\n",
      "Loaded 101/4767 samples.\n",
      "Processed orientation-es-train.tsv, created train, val, and test datasets of size 81, 10, and 10 respectively.\n",
      "Loaded 310/740 samples.\n",
      "Processed orientation-fi-train.tsv, created train, val, and test datasets of size 248, 31, and 31 respectively.\n",
      "Loaded 2251/10861 samples.\n",
      "Processed orientation-no-train.tsv, created train, val, and test datasets of size 1801, 225, and 225 respectively.\n",
      "Loaded 1026/4136 samples.\n",
      "Processed orientation-cz-train.tsv, created train, val, and test datasets of size 821, 103, and 102 respectively.\n",
      "Loaded 183/798 samples.\n",
      "Processed orientation-lv-train.tsv, created train, val, and test datasets of size 147, 18, and 18 respectively.\n",
      "Loaded 2100/6507 samples.\n",
      "Processed orientation-hr-train.tsv, created train, val, and test datasets of size 1680, 210, and 210 respectively.\n",
      "Loaded 2064/5657 samples.\n",
      "Processed orientation-nl-train.tsv, created train, val, and test datasets of size 1652, 206, and 206 respectively.\n",
      "Loaded 667/2220 samples.\n",
      "Processed orientation-be-train.tsv, created train, val, and test datasets of size 534, 67, and 66 respectively.\n",
      "Loaded 659/3069 samples.\n",
      "Processed orientation-dk-train.tsv, created train, val, and test datasets of size 528, 66, and 65 respectively.\n",
      "Loaded 322/2518 samples.\n",
      "Processed orientation-si-train.tsv, created train, val, and test datasets of size 258, 32, and 32 respectively.\n",
      "Loaded 113/533 samples.\n",
      "Processed orientation-is-train.tsv, created train, val, and test datasets of size 91, 11, and 11 respectively.\n",
      "Loaded 1641/5489 samples.\n",
      "Processed orientation-pl-train.tsv, created train, val, and test datasets of size 1313, 164, and 164 respectively.\n",
      "Loaded 1485/2293 samples.\n",
      "Processed orientation-fr-train.tsv, created train, val, and test datasets of size 1189, 148, and 148 respectively.\n",
      "Loaded 584/3464 samples.\n",
      "Processed orientation-pt-train.tsv, created train, val, and test datasets of size 468, 58, and 58 respectively.\n",
      "Loaded 275/2545 samples.\n",
      "Processed orientation-ua-train.tsv, created train, val, and test datasets of size 221, 27, and 27 respectively.\n",
      "Loaded 1041/9789 samples.\n",
      "Processed orientation-rs-train.tsv, created train, val, and test datasets of size 833, 104, and 104 respectively.\n",
      "Loaded 85/2077 samples.\n",
      "Processed orientation-es-ct-train.tsv, created train, val, and test datasets of size 69, 8, and 8 respectively.\n",
      "Loaded 506/3438 samples.\n",
      "Processed orientation-at-train.tsv, created train, val, and test datasets of size 405, 51, and 50 respectively.\n",
      "Loaded 963/8425 samples.\n",
      "Processed orientation-se-train.tsv, created train, val, and test datasets of size 771, 96, and 96 respectively.\n",
      "Loaded 15043/24239 samples.\n",
      "Processed orientation-gb-train.tsv, created train, val, and test datasets of size 12035, 1504, and 1504 respectively.\n",
      "Loaded 604/3367 samples.\n",
      "Processed orientation-it-train.tsv, created train, val, and test datasets of size 484, 60, and 60 respectively.\n",
      "Loaded 1516/2580 samples.\n",
      "Processed orientation-ee-train.tsv, created train, val, and test datasets of size 1213, 152, and 151 respectively.\n",
      "Processed all files, created train, val, and test datasets of size 31411, 3922, and 3914 respectively.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from dataset import MyDataset\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "from typing import List\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "english_speaking_countries = ['gb']\n",
    "\n",
    "DATA_DIR = \"data/orientation\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"distilbert-base-cased\"\n",
    "OUTPUT_DIR = \"data/torch/orientation\"\n",
    "\n",
    "def load_data(file_path: str):\n",
    "    '''\n",
    "    Loads specified dataset and returns lists of columns\n",
    "    '''\n",
    "    country_code = file_path.split('-')[1]  # Extract country code from filename\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, delimiter='\\t', quoting=3, on_bad_lines='skip')\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if country_code in english_speaking_countries:\n",
    "        df['text_combined'] = df['text']\n",
    "    else:\n",
    "        df['text_combined'] = df['text_en']\n",
    "    # Drop rows where 'text_combined' is NaN or empty\n",
    "    df = df.dropna(subset=['text_combined'])\n",
    "    df = df[df['text_combined'] != '']\n",
    "    df['file_path'] = file_path  # Add file path information\n",
    "    return list(df['id']), list(df['speaker']), list(df['sex']), list(df['text']), list(df['text_combined']), list(df['label'])\n",
    "\n",
    "def train_val_test_split_country(data: MyDataset, val_size:float = 0.1, test_size:float = 0.1, random_state:int = 42):\n",
    "    train_size = 1 - test_size - val_size\n",
    "    train_data, val_data, test_data = random_split(data, [train_size, val_size, test_size], \\\n",
    "                                                   generator=torch.Generator().manual_seed(random_state))\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = [], [], []\n",
    "for filename in os.listdir(DATA_DIR):\n",
    "    if filename.endswith(\".tsv\"):\n",
    "        file_path = os.path.join(DATA_DIR, filename)\n",
    "        ids, speakers, sexes, texts, texts_en, labels = load_data(file_path)\n",
    "        df = MyDataset(\n",
    "            ids=ids,\n",
    "            speakers=speakers,\n",
    "            sexes=sexes,\n",
    "            texts=texts,\n",
    "            texts_en=texts_en,\n",
    "            labels=labels,\n",
    "            device=DEVICE,\n",
    "            model_name=MODEL_NAME\n",
    "        )\n",
    "        train_df, val_df, test_df = train_val_test_split_country(df)\n",
    "        train_dataset.append(train_df)\n",
    "        val_dataset.append(val_df)\n",
    "        test_dataset.append(test_df)\n",
    "        torch.save(train_df, os.path.join(OUTPUT_DIR, f\"train_dataset_{filename.replace('-train.tsv', '.pt')}\"))\n",
    "        torch.save(val_df, os.path.join(OUTPUT_DIR, f\"val_dataset_{filename.replace('-train.tsv', '.pt')}\"))\n",
    "        torch.save(test_df, os.path.join(OUTPUT_DIR, f\"test_dataset_{filename.replace('-train.tsv', '.pt')}\"))\n",
    "        breakpoint()\n",
    "        print(f\"Processed {filename}, created train, val, and test datasets of size {len(train_df)}, {len(val_df)}, and {len(test_df)} respectively.\")\n",
    "\n",
    "train_dataset = torch.utils.data.ConcatDataset(train_dataset)\n",
    "val_dataset = torch.utils.data.ConcatDataset(val_dataset)\n",
    "test_dataset = torch.utils.data.ConcatDataset(test_dataset)\n",
    "\n",
    "torch.save(train_dataset, os.path.join(OUTPUT_DIR, \"train_dataset_all.pt\"))\n",
    "torch.save(val_dataset, os.path.join(OUTPUT_DIR, \"val_dataset_all.pt\"))\n",
    "torch.save(test_dataset, os.path.join(OUTPUT_DIR, \"test_dataset_all.pt\"))\n",
    "\n",
    "print(f\"Processed all files, created train, val, and test datasets of size {len(train_dataset)}, {len(val_dataset)}, and {len(test_dataset)} respectively.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
